{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Understanding Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Supervised Fine-Tuning is a machine learning technique where we take a **pre-trained language model** (like Llama 3) and teach it to perform specific tasks by showing it examples.\n",
    "\n",
    "Think of it like this:\n",
    "- **Pre-trained model** = A student who has read millions of books and knows general knowledge\n",
    "- **Fine-tuning** = Teaching that student to become an expert in Nigerian government services by showing them question-answer pairs\n",
    "\n",
    "**How it works:**\n",
    "1. We start with Llama 3 (already trained on internet text)\n",
    "2. We show it examples: \"Question: How do I register a business?\" ‚Üí \"Answer: To register...\"\n",
    "3. The model learns the pattern and adapts its knowledge\n",
    "4. After training, it can answer similar questions it hasn't seen before\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why LoRA (Low-Rank Adaptation)?**\n",
    "- Training all 8 billion parameters is slow and expensive\n",
    "- LoRA only trains ~1% of parameters by adding small \"adapter\" layers\n",
    "- Result: 2x faster, uses 60% less memory, but maintains quality\n",
    "\n",
    "**Features:**\n",
    "- 2x faster training with Unsloth\n",
    "- 60% less memory usage\n",
    "- 4-bit quantization support\n",
    "- LoRA efficient fine-tuning\n",
    "\n",
    "**Requirements:**\n",
    "- GPU Runtime (T4, V100, or A100)\n",
    "- ~15GB GPU memory\n",
    "- training_data.json file uploaded to Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_header"
   },
   "source": [
    "## üì¶ Step 1: Install Required Libraries\n",
    "\n",
    "This cell installs all the required libraries:\n",
    "- **Unsloth**: Makes training 2x faster and uses less memory\n",
    "- **TRL**: Provides the `SFTTrainer` for supervised fine-tuning\n",
    "- **Transformers**: HuggingFace library for working with language models\n",
    "- **PEFT**: Enables LoRA (parameter-efficient fine-tuning)\n",
    "- **bitsandbytes**: Enables 4-bit quantization to save memory\n",
    "\n",
    "**What is 4-bit quantization?**\n",
    "Instead of storing model weights as 32-bit numbers, we use 4-bit numbers. This reduces memory by 8x with minimal quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_header"
   },
   "source": [
    "## üìö Step 2: Import Libraries\n",
    "\n",
    "Import all required libraries and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "imports"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- CRITICAL FIX FOR \"torch.int1\" ERROR ---\n",
    "if not hasattr(torch, \"int1\"):\n",
    "    print(\"‚ö†Ô∏è Patching torch.int1 = torch.int8 to fix attribute error.\")\n",
    "    torch.int1 = torch.int8\n",
    "# -------------------------------------------\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "This is your control panel for training. Key settings:\n",
    "\n",
    "**Model Settings:**\n",
    "- `MODEL_NAME`: Which pre-trained model to start from\n",
    "- `MAX_SEQ_LENGTH`: Maximum tokens in one training example (1024 = ~750 words)\n",
    "- `LOAD_IN_4BIT`: Use 4-bit quantization to save memory\n",
    "\n",
    "**LoRA Settings:**\n",
    "- `LORA_R = 16`: Rank of LoRA adapters (higher = more parameters, slower but potentially better)\n",
    "- `LORA_ALPHA = 16`: Scaling factor (typically equals rank)\n",
    "- `LORA_DROPOUT = 0`: No dropout (regularization technique)\n",
    "\n",
    "**Training Settings:**\n",
    "- `MAX_STEPS = 60`: Train for 60 optimizer steps (quick test run)\n",
    "- `PER_DEVICE_BATCH_SIZE = 2`: Process 2 examples at once\n",
    "- `GRADIENT_ACCUMULATION_STEPS = 4`: Accumulate 4 batches before updating (effective batch = 2√ó4 = 8)\n",
    "- `LEARNING_RATE = 2e-4`: How big each training step is (0.0002)\n",
    "- `WARMUP_STEPS = 5`: Gradually increase learning rate for first 5 steps\n",
    "\n",
    "**Why these numbers?**\n",
    "- Smaller batch sizes use less memory\n",
    "- Gradient accumulation simulates larger batches\n",
    "- 60 steps is for testing; real training uses 500-5000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded!\n",
      "Effective batch size: 8\n",
      "Max training steps: 60\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - OPTIMIZED FOR SPEED\n",
    "# ============================================================================\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = \"unsloth/llama-3.1-8b-Instruct-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA settings (lower rank = faster training)\n",
    "LORA_R = 16  # Reduced from 64 for faster training\n",
    "LORA_ALPHA = 16  # Matches rank\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "# Training settings (optimized for speed)\n",
    "MAX_STEPS = 60  # Set max steps instead of epochs for testing\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 5\n",
    "LOGGING_STEPS = 1\n",
    "\n",
    "# Data settings\n",
    "DATA_FILE = \"training_data.json\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Max training steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model_header"
   },
   "source": [
    "This cell loads the Llama 3 model and tokenizer:\n",
    "\n",
    "**Tokenizer:**\n",
    "- Converts text into numbers (tokens) that the model understands\n",
    "- Example: \"Hello world\" ‚Üí [15339, 1917]\n",
    "- Each token represents a piece of text (word, subword, or character)\n",
    "\n",
    "**Model:**\n",
    "- Loaded in 4-bit quantization to save memory\n",
    "- `dtype=None`: Auto-detect best precision (FP16 for T4, BF16 for A100)\n",
    "- Pre-trained on trillions of tokens of internet text\n",
    "\n",
    "**Padding configuration:**\n",
    "- Models need all sequences in a batch to be the same length\n",
    "- `pad_token`: Special token used to fill shorter sequences\n",
    "- `padding_side = \"right\"`: Add padding at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer loaded successfully!\n",
      "Model dtype: torch.float16\n",
      "Vocabulary size: 128256\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect (Float16 for T4, Bfloat16 for A100)\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Configure tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora_header"
   },
   "source": [
    "This cell adds LoRA (Low-Rank Adaptation) layers to the model:\n",
    "\n",
    "**What are we adding?**\n",
    "- Small trainable matrices to attention and feed-forward layers\n",
    "- Target modules: `q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj`\n",
    "- These are the key computation layers in each transformer block\n",
    "\n",
    "**Why LoRA?**\n",
    "- Full fine-tuning: Train all 8B parameters (slow, memory-intensive)\n",
    "- LoRA: Train only ~42M parameters (0.92% of total)\n",
    "- Result: Much faster, less memory, similar quality\n",
    "\n",
    "**Key parameters:**\n",
    "- `r=16`: Rank of adaptation matrices (controls capacity)\n",
    "- `lora_alpha=16`: Scaling factor for LoRA updates\n",
    "- `use_gradient_checkpointing`: Trade compute for memory (slower but uses less RAM)\n",
    "\n",
    "After this cell, only the LoRA weights will be updated during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lora"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters added!\n",
      "Total parameters: 4,582,543,360\n",
      "Trainable parameters: 41,943,040 (0.92%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding LoRA adapters...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_header"
   },
   "source": [
    "\n",
    "\n",
    "## üìä Step 6: Load and Prepare Training Data\n",
    "\n",
    "Load your training_data.json file and split into three sets:\n",
    "\n",
    "**Three-way split:**\n",
    "- **Training (80%)**: Model learns from these examples\n",
    "- **Validation (10%)**: Monitor training progress, prevent overfitting\n",
    "- **Test (10%)**: Final evaluation on completely unseen data\n",
    "\n",
    "**Why split this way?**\n",
    "- **Train**: The model sees these during training\n",
    "- **Validation**: Check if model is generalizing (not memorizing)\n",
    "- **Test**: Hold out for final evaluation after training is complete\n",
    "\n",
    "**Data format:**\n",
    "Each example is converted to chat format with:\n",
    "- **System**: Instructions for the model's behavior\n",
    "- **User**: The question being asked\n",
    "- **Assistant**: The correct answer to learn from\n",
    "\n",
    "This structure teaches the model to respond appropriately to Nigerian government service questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from training_data.json...\n",
      "‚úÖ Loaded 6181 samples from JSON\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "Total samples: 6,181\n",
      "Training samples: 4,944 (80.0%)\n",
      "Validation samples: 618 (10.0%)\n",
      "Test samples: 619 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading training data from {DATA_FILE}...\")\n",
    "\n",
    "# Load JSON data\n",
    "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(raw_data)} samples from JSON\")\n",
    "\n",
    "# Convert to chat format for fine-tuning\n",
    "def convert_to_chat_format(item):\n",
    "    \"\"\"\n",
    "    Convert each item to chat format with system, user, and assistant messages.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant for Nigerian government services and agencies. Provide accurate information and include relevant contact details when available.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": item[\"question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": item[\"answer\"]\n",
    "            }\n",
    "        ],\n",
    "        \"agency\": item.get(\"agency\", \"Unknown\")\n",
    "    }\n",
    "\n",
    "# Convert all data\n",
    "formatted_data = [convert_to_chat_format(item) for item in raw_data]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(formatted_data)\n",
    "\n",
    "# Split into train (80%), validation (10%), and test (10%)\n",
    "train_size = int(len(df) * 0.8)\n",
    "val_size = int(len(df) * 0.1)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size + val_size]\n",
    "test_df = df[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Training samples: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format_header"
   },
   "source": [
    "## üîÑ Step 7: Format Dataset with Chat Template\n",
    "\n",
    "Apply Llama 3 chat template to format messages properly for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "format"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0856fda10b3e4b7990d6c22aa6547b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed115ecb1e6493b842a27cae351f0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Datasets formatted successfully!\n",
      "Training dataset: 5562 samples\n",
      "Validation dataset: 619 samples\n",
      "\n",
      "üìÑ Formatted example (first 500 chars):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant for Nigerian government services and agencies. Provide accurate information and include relevant contact details when available.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is APCON and what is its regulatory role in Nigeria‚Äôs advertising industry?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "APCON stands for the Advertisin...\n"
     ]
    }
   ],
   "source": [
    "def format_chat_template(example):\n",
    "    \"\"\"Apply chat template to format messages for training.\"\"\"\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "print(\"Formatting training dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "\n",
    "print(\"Formatting validation dataset...\")\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "val_dataset = val_dataset.map(format_chat_template)\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets formatted successfully!\")\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "print(\"\\nüìÑ Formatted example (first 500 chars):\")\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_config_header"
   },
   "source": [
    "## ‚öôÔ∏è Step 8: Configure Training Arguments\n",
    "\n",
    "This cell configures how the model will be trained:\n",
    "\n",
    "**Training Schedule:**\n",
    "- `max_steps=60`: Stop after 60 training steps (quick test)\n",
    "- For real training, use `num_train_epochs=3` instead of `max_steps`\n",
    "\n",
    "**Batch Configuration:**\n",
    "- `per_device_train_batch_size=2`: Process 2 examples per GPU\n",
    "- `gradient_accumulation_steps=4`: Accumulate gradients for 4 batches\n",
    "- **Effective batch size = 2 √ó 4 = 8 examples per update**\n",
    "\n",
    "**Optimization:**\n",
    "- `learning_rate=2e-4`: Step size for parameter updates (0.0002)\n",
    "- `warmup_steps=5`: Gradually increase LR for stability\n",
    "- `optim=\"adamw_8bit\"`: Memory-efficient AdamW optimizer\n",
    "- `lr_scheduler_type=\"linear\"`: Learning rate decreases linearly\n",
    "\n",
    "**Precision:**\n",
    "- `fp16` or `bf16`: Use 16-bit floating point (2x faster, half the memory)\n",
    "- BF16 is better for training but only available on newer GPUs\n",
    "\n",
    "**Other:**\n",
    "- `seed=3407`: For reproducibility (same results every time)\n",
    "- `report_to=\"none\"`: Don't log to WandB/TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "train_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training configuration created!\n",
      "Effective batch size: 8\n",
      "Total training steps: 60\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training schedule\n",
    "    max_steps=MAX_STEPS,  # Using max_steps instead of num_epochs\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Precision\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Other\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # Add this line!\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created!\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Total training steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trainer_header"
   },
   "source": [
    "## üéØ Step 9: Initialize Trainer\n",
    "This cell creates the SFTTrainer that handles the training loop:\n",
    "\n",
    "**What does SFTTrainer do?**\n",
    "1. Takes your formatted dataset\n",
    "2. Tokenizes text into numbers\n",
    "3. Creates batches of examples\n",
    "4. Feeds batches through the model\n",
    "5. Computes loss (how wrong the predictions are)\n",
    "6. Updates model weights to reduce loss\n",
    "7. Repeats until training is complete\n",
    "\n",
    "**Key parameters:**\n",
    "- `dataset_text_field=\"text\"`: Column containing formatted chat text\n",
    "- `max_seq_length=1024`: Truncate sequences longer than 1024 tokens\n",
    "- `dataset_num_proc=2`: Use 2 CPU cores for data processing\n",
    "- `packing=False`: Don't pack multiple examples into one sequence\n",
    "\n",
    "**What is packing?**\n",
    "- With packing: Combine short examples to fill max_seq_length ‚Üí More efficient\n",
    "- Without packing: One example per sequence ‚Üí Simpler, easier to debug\n",
    "- For beginners, `packing=False` is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "trainer"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521f8e56b3aa45ae9b99b7dce6133564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/5562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a380243fb04baaa67c621a55801901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,           # ‚Üê keeps eval working\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "## üöÄ Step 10: Start Training\n",
    "\n",
    "Execute the training loop. This will:\n",
    "- Train for the specified number of epochs\n",
    "- Evaluate on validation set periodically\n",
    "- Save checkpoints\n",
    "- Track training loss\n",
    "\n",
    "**This may take 30 minutes to several hours depending on dataset size and GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "train"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,562 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.746400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.915800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.822000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ Training completed!\n",
      "Final training loss: 1.1324\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_header"
   },
   "source": [
    "## üìä Step 11: Final Evaluation\n",
    "\n",
    "This cell evaluates the model on the validation set:\n",
    "\n",
    "**Why evaluate?**\n",
    "- Training loss shows how well the model fits training data\n",
    "- Validation loss shows if the model generalizes to new examples\n",
    "- If training loss ‚Üì but validation loss ‚Üë = overfitting (memorizing, not learning)\n",
    "\n",
    "**What is `eval_loss`?**\n",
    "- Average loss on validation examples\n",
    "- Should be close to final training loss\n",
    "- If much higher: Model is overfitting\n",
    "\n",
    "**Other metrics:**\n",
    "- `eval_runtime`: How long evaluation took\n",
    "- `eval_samples_per_second`: Throughput\n",
    "- `eval_steps_per_second`: Processing speed\n",
    "\n",
    "**Good results:**\n",
    "- `eval_loss` close to training loss (within 0.1-0.2)\n",
    "- `eval_loss` < 1.0 for good quality responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eval"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation...\n",
      "Validation dataset columns: ['messages', 'agency', 'text']\n",
      "Sample text length: 1288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 02:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Results:\n",
      "  eval_loss: 0.9315\n",
      "  eval_runtime: 139.2768\n",
      "  eval_samples_per_second: 4.4440\n",
      "  eval_steps_per_second: 0.5600\n",
      "  epoch: 0.0863\n",
      "\n",
      "‚úÖ Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running final evaluation...\")\n",
    "\n",
    "# Safety check\n",
    "print(\"Validation dataset columns:\", val_dataset.column_names)\n",
    "print(\"Sample text length:\", len(val_dataset[0][\"text\"]))\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_header"
   },
   "source": [
    "## üíæ Step 12: Save Fine-tuned Model\n",
    "\n",
    "Save both LoRA adapters and merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "save"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters saved to: ./outputs/lora_adapters\n",
      "\n",
      "Saving merged model (this may take a few minutes)...\n",
      "Found HuggingFace hub cache directory: /teamspace/studios/this_studio/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 26800.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:18<00:00, 19.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/teamspace/studios/this_studio/outputs/merged_model`\n",
      "‚úÖ Merged model saved to: ./outputs/merged_model\n",
      "\n",
      "======================================================================\n",
      "MODEL SAVED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save LoRA adapters (small, ~50-100MB)\n",
    "lora_path = f\"{OUTPUT_DIR}/lora_adapters\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"‚úÖ LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Save merged model (large, ~16GB)\n",
    "print(\"\\nSaving merged model (this may take a few minutes)...\")\n",
    "merged_path = f\"{OUTPUT_DIR}/merged_model\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    merged_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"‚úÖ Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_header"
   },
   "source": [
    "## üß™ Step 13: Test the Fine-tuned Model\n",
    "\n",
    "Test the fine-tuned model with sample questions from your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fine-tuned model...\n",
      "\n",
      "======================================================================\n",
      "Test 1: How can I register my business in Nigeria?\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: Business registration in Nigeria typically starts with choosing a name and then registering the business with the Corporate Affairs Commission (CAC). The CAC website provides online registration tools and guidelines. You can start by checking name availability, then proceed to register your company through the CAC portal, including filing documents and making payments. For more information, visit the CAC website (www.cac.gov.ng) and follow the registration process.\n",
      "\n",
      "Contact Information:\n",
      "Agency: Corporate Affairs Commission (CAC)\n",
      "Official Address: Plot 420, Tigris Crescent,\n",
      "Off Aguiyi Ironsi Street,\n",
      "Maitama, Abuja.\n",
      "Nigeria.\n",
      "Official Email: cservice@cac.gov.ng\n",
      "Official Phone: (+234) 708 062 9000\n",
      "Official Website: https://www.cac.gov.ng/\n",
      "\n",
      "======================================================================\n",
      "Test 2: What are the requirements for obtaining a business license?\n",
      "======================================================================\n",
      "\n",
      "Response: Applicants must meet specific licensing requirements for their business type, including registration with CAC, payment of relevant fees, and compliance with CAC regulations. The license is issued for a specified period, typically 5 years, after which renewal is required. CAC may revoke licenses for non-compliance.\n",
      "\n",
      "Contact Information:\n",
      "Agency: Corporate Affairs Commission (CAC)\n",
      "Official Address: Plot 420, Tigris Crescent,\n",
      "Off Aguiyi Ironsi Street,\n",
      "Maitama, Abuja.\n",
      "Nigeria.\n",
      "Official Email: cservice@cac.gov.ng\n",
      "Official Phone: (+234) 708 062 9000\n",
      "Official Website: https://www.cac.gov.ng/\n",
      "\n",
      "======================================================================\n",
      "Test 3: How do I get a Loan from CBN\n",
      "======================================================================\n",
      "\n",
      "Response: You can apply for loans from the Bank‚Äôs approved products and channels, including the CBN Intervention Funds, the Bank‚Äôs Special Intervention Funds, and the Bank‚Äôs other facilities such as the Agricultural Credit Support Loan and the Small Medium Enterprises Loan. Contact your bank or visit the CBN website for more information on eligibility, application procedures, and interest rates.\n",
      "\n",
      "Contact Information:\n",
      "Agency: Central Bank Of Nigeria (CBN)\n",
      "Official Address: Central Bank of Nigeria,\n",
      "Plot 33, Abubakar Tafawa Balewa Way\n",
      "Central Business District,\n",
      "Cadastral Zone,\n",
      "Abuja, Federal Capital Territory,\n",
      "Nigeria\n",
      "P.M.B. 0187,\n",
      "Garki, Abuja.\n",
      "Official Email: contactcbn@cbn.gov.ng\n",
      "Official Phone: (+234) 817 665 7060\n",
      "Official Website: https://www.cbn.gov.ng/\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Testing completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the fine-tuned model...\")\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"How can I register my business in Nigeria?\",\n",
    "    \"What are the requirements for obtaining a business license?\",\n",
    "    \"How do I get a Loan from CBN\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Test {i}: {question}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for Nigerian government services and agencies. Provide accurate information and concise information\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Format and tokenize\n",
    "    test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"assistant\" in response:\n",
    "        assistant_response = response.split(\"assistant\")[-1].strip()\n",
    "        print(f\"\\nResponse: {assistant_response}\")\n",
    "    else:\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Testing completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This cell evaluates your fine-tuned model using quantitative metrics:\n",
    "\n",
    "**ROUGE Metrics** (Recall-Oriented Understudy for Gisting Evaluation):\n",
    "- Measures overlap between generated response and reference answer\n",
    "- **ROUGE-1**: Unigram (single word) overlap\n",
    "- **ROUGE-2**: Bigram (two consecutive words) overlap\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "\n",
    "**How ROUGE works:**\n",
    "- Reference: \"The capital of Nigeria is Abuja\"\n",
    "- Response: \"Nigeria's capital city is Abuja\"\n",
    "- ROUGE-1: High (matches: capital, Nigeria, Abuja)\n",
    "- ROUGE-2: Medium (matches: \"is Abuja\")\n",
    "- ROUGE-L: Medium (longest common sequence)\n",
    "\n",
    "**Interpreting scores** (0-1 scale):\n",
    "- **>0.3**: Good overlap, relevant response\n",
    "- **0.2-0.3**: Moderate overlap, partially correct\n",
    "- **<0.2**: Low overlap, may be wrong or off-topic\n",
    "\n",
    "**Note:** ROUGE measures n-gram overlap, not semantic meaning. A response can be correct but score low if phrased differently.\n",
    "\n",
    "**Evaluation process:**\n",
    "1. Load test dataset (unseen during training)\n",
    "2. For each question, generate model response\n",
    "3. Compare response to reference answer\n",
    "4. Calculate ROUGE scores\n",
    "5. Average across all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.20.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=d33a006ae27a669fd5941c1a2ae4b1e90e74bfc09eb7da95b2df949b6761cc54\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate \n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10 samples from TEST SET...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:51<00:00, 11.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUATION RESULTS (Test Set - Unseen Data)\n",
      "============================================================\n",
      "Samples evaluated: 10\n",
      "\n",
      "ROUGE Scores (0-1, higher is better):\n",
      "  ROUGE-1: 0.424  (word overlap)\n",
      "  ROUGE-2: 0.235  (phrase overlap)\n",
      "  ROUGE-L: 0.358  (sentence overlap)\n",
      "\n",
      "Interpretation:\n",
      "  ‚úÖ Good word overlap with reference answers\n",
      "  ‚úÖ Good sentence structure similarity\n",
      "============================================================\n",
      "\n",
      "üìù Sample Responses:\n",
      "\n",
      "Example 1:\n",
      "Q: How does the Finance Act, 2020 reclassify company sizes and define primary agricultural production?...\n",
      "\n",
      "üìñ Reference: The Act defines new categories: (a) medium sized company as one with gross turnover between N25,000,000 and N100,000,000 per annum; (b) small sized company as one with gross turnover of N25,000,000 or...\n",
      "\n",
      "ü§ñ Model: The Finance Act, 2020 amends the Companies and Allied Matters Act to classify companies by size (Small, Medium, and Large) based on turnover, assets, and employees. Small companies are defined as thos...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Q: What is the significance of new section 6A in Personal Income Tax under the Finance Act, 2020?...\n",
      "\n",
      "üìñ Reference: New section 6A establishes that if a trade or business conducted outside Nigeria provides services (technical, management, consultancy, or professional) to a person resident in Nigeria, the profits de...\n",
      "\n",
      "ü§ñ Model: New section 6A in the Finance Act, 2020 introduces a penalty of 30% for non-filing of tax returns or failure to comply with tax obligations. The section applies to individual taxpayers with assessable...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Q: Where can I find more information about SMEDAN programs, events, and updates?...\n",
      "\n",
      "üìñ Reference: For more information on SMEDAN programs, trainings, and opportunities, visit the SMEDAN website at https://smedan.gov.ng and regularly check the registration portal at http://smedanregister.ng. You ca...\n",
      "\n",
      "ü§ñ Model: For the latest information on SMEDAN programs, events, and updates, visit the SMEDAN official website at https://www.smedan.gov.ng/. This platform provides details on registration, training, and other...\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Simple Model Evaluation on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Number of samples to test\n",
    "NUM_EVAL_SAMPLES = min(10, len(test_df))  # Use test set\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Evaluating {NUM_EVAL_SAMPLES} samples from TEST SET...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate samples from TEST set (unseen during training)\n",
    "for i in tqdm(range(NUM_EVAL_SAMPLES)):\n",
    "    sample = test_df.iloc[i]\n",
    "    \n",
    "    # Get question and reference answer\n",
    "    question = sample['messages'][1]['content']  # User message\n",
    "    reference = sample['messages'][2]['content']  # Assistant message\n",
    "    \n",
    "    # Create prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for Nigerian government services and agencies. Provide accurate information and include relevant contact details when available.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Format and generate\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Extract response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    predictions.append(response)\n",
    "    references.append(reference)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS (Test Set - Unseen Data)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Samples evaluated: {NUM_EVAL_SAMPLES}\")\n",
    "print(f\"\\nROUGE Scores (0-1, higher is better):\")\n",
    "print(f\"  ROUGE-1: {rouge_results['rouge1']:.3f}  (word overlap)\")\n",
    "print(f\"  ROUGE-2: {rouge_results['rouge2']:.3f}  (phrase overlap)\")\n",
    "print(f\"  ROUGE-L: {rouge_results['rougeL']:.3f}  (sentence overlap)\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if rouge_results['rouge1'] > 0.3:\n",
    "    print(\"Good word overlap with reference answers\")\n",
    "elif rouge_results['rouge1'] > 0.2:\n",
    "    print(\"Fair overlap - model is learning but could improve\")\n",
    "else:\n",
    "    print(\"Low overlap - model needs more training\")\n",
    "\n",
    "if rouge_results['rougeL'] > 0.25:\n",
    "    print(\"Good sentence structure similarity\")\n",
    "else:\n",
    "    print(\"Responses differ significantly in structure\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show 3 examples\n",
    "print(\"\\nüìù Sample Responses:\\n\")\n",
    "for i in range(min(3, NUM_EVAL_SAMPLES)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Q: {test_df.iloc[i]['messages'][1]['content'][:100]}...\")\n",
    "    print(f\"\\nüìñ Reference: {references[i][:200]}...\")\n",
    "    print(f\"\\nü§ñ Model: {predictions[i][:200]}...\")\n",
    "    print(\"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_header"
   },
   "source": [
    "## ‚òÅÔ∏è Step 14: (Optional) Push to Hugging Face Hub\n",
    "\n",
    "Upload your model to Hugging Face for sharing and deployment.\n",
    "\n",
    "**Uncomment and run if you want to upload.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "login()\n",
    "\n",
    "# Push LoRA adapters\n",
    "print(\"Pushing LoRA adapters to Hugging Face...\")\n",
    "model.push_to_hub(HF_MODEL_NAME, token=True, private=False)\n",
    "tokenizer.push_to_hub(HF_MODEL_NAME, token=True, private=False)\n",
    "\n",
    "# Optionally push merged model (takes longer)\n",
    "# print(\"Pushing merged model to Hugging Face...\")\n",
    "# merged_model.push_to_hub(f\"{HF_MODEL_NAME}-merged\", token=True, private=False)\n",
    "\n",
    "print(f\"‚úÖ Model pushed to: https://huggingface.co/{HF_MODEL_NAME}\")\n",
    "\n",
    "print(\"Upload cell ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## üì• Step 15: Download Model Files\n",
    "\n",
    "Download the trained model to your local machine.\n",
    "\n",
    "This will create a zip file you can download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_direct"
   },
   "outputs": [],
   "source": [
    "# Download directly in Colab\n",
    "from google.colab import files\n",
    "\n",
    "files.download('llama3_finetuned_model.zip')\n",
    "print(\"‚úÖ Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "\n",
    "2. **Adjust hyperparameters**: If results aren't optimal, try:\n",
    "   - Increasing epochs (NUM_EPOCHS)\n",
    "   - Adjusting learning rate (LEARNING_RATE)\n",
    "   - Changing LoRA rank (LORA_R)\n",
    "3. **Deploy**: Use the model in your application\n",
    "4. **Share**: Upload to Hugging Face Hub\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
